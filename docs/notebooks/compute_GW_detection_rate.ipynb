{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "83c09708",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import os\n",
    "import sys\n",
    "import warnings\n",
    "from functools import partial\n",
    "\n",
    "import astropy.units as u\n",
    "import lal.series\n",
    "import numpy as np\n",
    "import requests\n",
    "from astropy import cosmology, units\n",
    "from astropy.cosmology._utils import vectorize_redshift_method\n",
    "from astropy.table import Table\n",
    "from astropy.units import dimensionless_unscaled\n",
    "from ligo.lw import ligolw\n",
    "from ligo.lw import utils as ligolw_utils\n",
    "from ligo.skymap.bayestar.filter import sngl_inspiral_psd\n",
    "from ligo.skymap.util import progress_map\n",
    "from scipy import integrate, optimize, special, stats\n",
    "from scipy.integrate import fixed_quad, quad\n",
    "from scipy.interpolate import interp1d\n",
    "from scipy.optimize import root_scalar\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52c79b52",
   "metadata": {},
   "source": [
    "# Download and prepare GWTC-3 distribustion as farah.h5 file "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ecff5483",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_file(url, filename):\n",
    "    \"\"\"Download a file from a URL and save it locally.\"\"\"\n",
    "    try:\n",
    "        response = requests.get(url, stream=True)\n",
    "        response.raise_for_status()\n",
    "        with open(filename, \"wb\") as f:\n",
    "            for chunk in response.iter_content(chunk_size=8192):\n",
    "                f.write(chunk)\n",
    "        return filename\n",
    "    except Exception as e:\n",
    "        print(f\"Error downloading file: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dfa82e8",
   "metadata": {},
   "source": [
    "### Define Paths and Filenames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8c508e3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = \"../../scenarios\"\n",
    "farah_file = os.path.join(output_dir, \"farah.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c28dda8",
   "metadata": {},
   "source": [
    "## Download the File if Needed\n",
    "\n",
    "Download the GWTC-3 PDB file (see https://dcc.ligo.org/LIGO-T2100512/public/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ea9dd5f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_file = None\n",
    "if not os.path.exists(farah_file):\n",
    "    file_url = \"https://dcc.ligo.org/LIGO-T2100512/public/O1O2O3all_mass_h_iid_mag_iid_tilt_powerlaw_redshift_maxP_events_all.h5\"\n",
    "    file_name = os.path.join(output_dir, file_url.split(\"/\")[-1])\n",
    "    input_file = download_file(file_url, file_name)\n",
    "\n",
    "    if input_file is None:\n",
    "        logging.error(\"Failed to download the required file.\")\n",
    "        sys.exit(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d4ed8fd",
   "metadata": {},
   "source": [
    "## Read, Transform, and Save Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f2802114",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "farah.h5 already exists at: ../../scenarios/farah.h5\n"
     ]
    }
   ],
   "source": [
    "if not os.path.exists(farah_file):\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.simplefilter(\"ignore\")\n",
    "\n",
    "        # Read the original file and save only the relevant columns\n",
    "        data = Table.read()\n",
    "        Table(\n",
    "            {\n",
    "                \"mass1\": data[\"mass_1\"],\n",
    "                \"mass2\": data[\"mass_2\"],\n",
    "                \"spin1z\": data[\"a_1\"] * data[\"cos_tilt_1\"],\n",
    "                \"spin2z\": data[\"a_2\"] * data[\"cos_tilt_2\"],\n",
    "            }\n",
    "        ).write(farah_file, overwrite=True)\n",
    "\n",
    "else:\n",
    "    print(f\"farah.h5 already exists at: {farah_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70dc6f81",
   "metadata": {},
   "source": [
    "## Mass threshold for neutron stars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f1063f24",
   "metadata": {},
   "outputs": [],
   "source": [
    "ns_mass = 3.0  # Solar masses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0c996de",
   "metadata": {},
   "source": [
    "## Clean Up and Print Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "124c656d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary of Compact Binary Types\n",
      "---------------------------------\n",
      "Number of BNS   : 892762\n",
      "Number of NSBH  : 35962\n",
      "Number of BBH   : 71276\n",
      "---------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Remove the original large file if it exists\n",
    "if input_file is not None and os.path.exists(input_file):\n",
    "    os.remove(input_file)\n",
    "    logging.info(f\"Removed temporary file: {input_file}\")\n",
    "\n",
    "# Reload the processed file\n",
    "if os.path.exists(farah_file):\n",
    "    data = Table.read(farah_file)\n",
    "\n",
    "    # Compute statistics for binary systems\n",
    "    n_bns = len(data[data[\"mass1\"] < ns_mass])\n",
    "    n_nsbh = len(data[(data[\"mass1\"] >= ns_mass) & (data[\"mass2\"] < ns_mass)])\n",
    "    n_bbh = len(data[data[\"mass2\"] >= ns_mass])\n",
    "\n",
    "    output = (\n",
    "        \"Summary of Compact Binary Types\\n\"\n",
    "        \"---------------------------------\\n\"\n",
    "        f\"Number of BNS   : {n_bns}\\n\"\n",
    "        f\"Number of NSBH  : {n_nsbh}\\n\"\n",
    "        f\"Number of BBH   : {n_bbh}\\n\"\n",
    "        \"---------------------------------\\n\"\n",
    "    )\n",
    "    print(output)\n",
    "else:\n",
    "    print(\"farah.h5 not found. Please check the previous steps.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55cc65be",
   "metadata": {},
   "source": [
    "## SNR Selection Helper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ae7736fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_decisive_snr(snrs, min_triggers):\n",
    "    \"\"\"\n",
    "    Return the SNR value that decides if an event is detectable (the min_triggers-th highest SNR).\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    snrs : list or array-like\n",
    "        List of SNRs (floats).\n",
    "    min_triggers : int\n",
    "        Minimum number of triggers to form a coincidence.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    decisive_snr : float\n",
    "        The SNR of the trigger that decides detectability.\n",
    "    \"\"\"\n",
    "    return sorted(snrs)[-min_triggers]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a504f27b",
   "metadata": {},
   "source": [
    "## Find First/Last Nonzero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f36896cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lo_hi_nonzero(x):\n",
    "    \"\"\"Return indices of the first and last nonzero elements of an array.\"\"\"\n",
    "    nonzero = np.flatnonzero(x)\n",
    "    return nonzero[0], nonzero[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7671aa17",
   "metadata": {},
   "source": [
    "## GWCosmo Class: Gravitational Wave Cosmology Tools"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37d15b9c",
   "metadata": {},
   "source": [
    "The `GWCosmo` class provides methods to calculate important cosmological figures of merit for gravitational wave (GW) astronomy. \n",
    "Given a cosmological model, it can estimate the maximum observable distance for GW sources, the sensitive volume out to a given redshift, and related quantities."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6631a812",
   "metadata": {},
   "source": [
    "### Main Methods\n",
    "\n",
    "- `z_at_snr(...)`:  \n",
    "  Computes the redshift at which a GW source achieves a specific signal-to-noise ratio (SNR) in a detector network.\n",
    "\n",
    "- `get_max_z(...)`:  \n",
    "  Returns the maximum redshift (distance) at which a source can be detected, given the detector sensitivity and source parameters.\n",
    "\n",
    "- `sensitive_volume(z)`:  \n",
    "  Calculates the sensitive comoving volume out to redshift `z`, useful for GW event rate calculations.\n",
    "\n",
    "- `sensitive_distance(z)`:  \n",
    "  Returns the “sensitive distance,” defined such that the sensitive volume equals that of a sphere with radius `d_s(z)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "85fb96d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GWCosmo:\n",
    "    \"\"\"Evaluate GW distance figures of merit for a given cosmology.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    cosmo : :class:`astropy.cosmology.FLRW`\n",
    "        The cosmological model.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, cosmology):\n",
    "        self.cosmo = cosmology\n",
    "\n",
    "    def z_at_snr(\n",
    "        self,\n",
    "        psds,\n",
    "        waveform,\n",
    "        f_low,\n",
    "        snr_threshold,\n",
    "        min_triggers,\n",
    "        mass1,\n",
    "        mass2,\n",
    "        spin1z,\n",
    "        spin2z,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Get redshift at which a waveform attains a given SNR.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        psds : list\n",
    "            List of :class:`lal.REAL8FrequencySeries` objects.\n",
    "        waveform : str\n",
    "            Waveform approximant name.\n",
    "        f_low : float\n",
    "            Low-frequency cutoff for template.\n",
    "        snr_threshold : float\n",
    "            Minimum single-detector SNR.\n",
    "        min_triggers : int\n",
    "            Minimum number of triggers to form a coincidence.\n",
    "        params : list\n",
    "            List of waveform parameters: mass1, mass2, spin1z, spin2z.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        comoving_distance : float\n",
    "            Comoving distance in Mpc.\n",
    "\n",
    "        \"\"\"\n",
    "        # Construct waveform\n",
    "        series = sngl_inspiral_psd(\n",
    "            waveform,\n",
    "            f_low=f_low,\n",
    "            mass1=mass1,\n",
    "            mass2=mass2,\n",
    "            spin1z=spin1z,\n",
    "            spin2z=spin2z,\n",
    "        )\n",
    "        i_lo, i_hi = lo_hi_nonzero(series.data.data)\n",
    "        log_f = np.log(series.f0 + series.deltaF * np.arange(i_lo, i_hi + 1))\n",
    "        log_f_lo = log_f[0]\n",
    "        log_f_hi = log_f[-1]\n",
    "        num = interp1d(\n",
    "            log_f,\n",
    "            np.log(series.data.data[i_lo : i_hi + 1]),\n",
    "            fill_value=-np.inf,\n",
    "            bounds_error=False,\n",
    "            assume_sorted=True,\n",
    "        )\n",
    "\n",
    "        denoms = []\n",
    "        for series in psds:\n",
    "            i_lo, i_hi = lo_hi_nonzero(\n",
    "                np.isfinite(series.data.data) & (series.data.data != 0)\n",
    "            )\n",
    "            log_f = np.log(series.f0 + series.deltaF * np.arange(i_lo, i_hi + 1))\n",
    "            denom = interp1d(\n",
    "                log_f,\n",
    "                log_f - np.log(series.data.data[i_lo : i_hi + 1]),\n",
    "                fill_value=-np.inf,\n",
    "                bounds_error=False,\n",
    "                assume_sorted=True,\n",
    "            )\n",
    "            denoms.append(denom)\n",
    "\n",
    "        def snr_at_z(z):\n",
    "            logzp1 = np.log(z + 1)\n",
    "\n",
    "            def integrand(log_f):\n",
    "                return [np.exp(num(log_f + logzp1) + denom(log_f)) for denom in denoms]\n",
    "\n",
    "            integrals, _ = fixed_quad(integrand, log_f_lo, log_f_hi - logzp1, n=1024)\n",
    "            snr = get_decisive_snr(np.sqrt(4 * integrals), min_triggers)\n",
    "            with np.errstate(divide=\"ignore\"):\n",
    "                snr /= self.cosmo.angular_diameter_distance(z).to_value(units.Mpc)\n",
    "            return snr\n",
    "\n",
    "        def root_func(z):\n",
    "            return snr_at_z(z) - snr_threshold\n",
    "\n",
    "        return root_scalar(root_func, bracket=(0, 1e3)).root\n",
    "\n",
    "    def get_max_z(\n",
    "        self,\n",
    "        psds,\n",
    "        waveform,\n",
    "        f_low,\n",
    "        snr_threshold,\n",
    "        min_triggers,\n",
    "        mass1,\n",
    "        mass2,\n",
    "        spin1z,\n",
    "        spin2z,\n",
    "        jobs=1,\n",
    "    ):\n",
    "        # Calculate the maximum distance on the grid.\n",
    "        params = [mass1, mass2, spin1z, spin2z]\n",
    "        shape = np.broadcast_shapes(*(param.shape for param in params))\n",
    "        result = list(\n",
    "            progress_map(\n",
    "                partial(\n",
    "                    self.z_at_snr, psds, waveform, f_low, snr_threshold, min_triggers\n",
    "                ),\n",
    "                *(param.ravel() for param in params),\n",
    "                jobs=jobs,\n",
    "            )\n",
    "        )\n",
    "        result = np.reshape(result, shape)\n",
    "\n",
    "        assert np.all(result >= 0), \"some redshifts are negative\"\n",
    "        assert np.all(np.isfinite(result)), \"some redshifts are not finite\"\n",
    "        return result\n",
    "\n",
    "    @vectorize_redshift_method\n",
    "    def _sensitive_volume_integral(self, z):\n",
    "        dh3_sr = self.cosmo.hubble_distance**3 / units.sr\n",
    "\n",
    "        def integrand(z):\n",
    "            result = self.cosmo.differential_comoving_volume(z)\n",
    "            result /= (1 + z) * dh3_sr\n",
    "            return result.to_value(dimensionless_unscaled)\n",
    "\n",
    "        result, _ = quad(integrand, 0, z)\n",
    "        return result\n",
    "\n",
    "    def sensitive_volume(self, z):\n",
    "        \"\"\"Sensitive volume :math:`V(z)` out to redshift :math:`z`.\n",
    "\n",
    "        Given a population of events that occur at a constant rate density\n",
    "        :math:`R` per unit comoving volume per unit proper time, the number of\n",
    "        observed events out to a redshift :math:`N(z)` over an observation time\n",
    "        :math:`T` is :math:`N(z) = R T V(z)`.\n",
    "        \"\"\"\n",
    "        dh3 = self.cosmo.hubble_distance**3\n",
    "        return 4 * np.pi * dh3 * self._sensitive_volume_integral(z)\n",
    "\n",
    "    def sensitive_distance(self, z):\n",
    "        r\"\"\"Sensitive distance as a function of redshift :math:`z`.\n",
    "\n",
    "        The sensitive distance is the distance :math:`d_s(z)` defined such that\n",
    "        :math:`V(z) = 4/3\\pi {d_s(z)}^3`, where :math:`V(z)` is the sensitive\n",
    "        volume.\n",
    "        \"\"\"\n",
    "        dh = self.cosmo.hubble_distance\n",
    "        return dh * np.cbrt(3 * self._sensitive_volume_integral(z))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b473d301",
   "metadata": {},
   "source": [
    "    # ----------------- Execution & Example -----------------\n",
    "\n",
    "##### 1. We load the noise Power Spectral Densities (PSDs) from the XML file.\n",
    "#####    These PSDs are used to compute signal-to-noise ratios (SNRs) across detectors.\n",
    "\n",
    "##### Initialize a new LIGO Light-Weight XML document\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "e6ded26d",
   "metadata": {},
   "outputs": [],
   "source": [
    "xmldoc = ligolw.Document()\n",
    "xmlroot = xmldoc.appendChild(ligolw.LIGO_LW())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27616a61",
   "metadata": {},
   "source": [
    "#### Initialize a new LIGO Light-Weight XML document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "3424f6bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<Swig Object of type 'tagREAL8FrequencySeries *' at 0x19dd3b130>,\n",
       " <Swig Object of type 'tagREAL8FrequencySeries *' at 0x19dd38f30>,\n",
       " <Swig Object of type 'tagREAL8FrequencySeries *' at 0x19dd382b0>,\n",
       " <Swig Object of type 'tagREAL8FrequencySeries *' at 0x19dd3afb0>]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reference_psd = \"../../scenarios/psds.xml\"\n",
    "with open(reference_psd, \"rb\") as f:\n",
    "    xmldoc = ligolw_utils.load_fileobj(f, contenthandler=lal.series.PSDContentHandler)\n",
    "    psds = list(lal.series.read_psd_xmldoc(xmldoc).values())\n",
    "\n",
    "psds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "449a3457",
   "metadata": {},
   "source": [
    "#### 2. We define the waveform model, the low-frequency cutoff, and the detection thresholds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d6f358e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "waveform = \"IMRPhenomD\"  # Chosen waveform approximant\n",
    "f_low = 25.0  # Low-frequency cutoff in Hz\n",
    "snr_threshold = 1  # SNR threshold for detection\n",
    "min_triggers = 1  # Minimum number of triggers (coincident detectors)\n",
    "jobs = 1  # Number of parallel jobs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d289badd",
   "metadata": {},
   "source": [
    "\n",
    "##### 3. We initialize the GWCosmo object with a cosmological model (Planck15 here).\n",
    "#####    This allows us to compute distances and volumes in a cosmological framework."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "35c8d8fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "gwcosmo = GWCosmo(getattr(cosmology, \"Planck15\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9f50acf",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "##### 4. We load a subset (first 5) of the binary neutron star / black hole samples \n",
    "#####    from an HDF5 file (`farah.h5`). Each sample includes mass1, mass2, spin1z, spin2z."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "4b42677f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><i>Table length=5</i>\n",
       "<table id=\"table6940940880\" class=\"table-striped table-bordered table-condensed\">\n",
       "<thead><tr><th>mass1</th><th>mass2</th><th>spin1z</th><th>spin2z</th></tr></thead>\n",
       "<thead><tr><th>float64</th><th>float64</th><th>float64</th><th>float64</th></tr></thead>\n",
       "<tr><td>1.6334784673051967</td><td>1.3810887548655666</td><td>0.0005726141112029242</td><td>-0.012026548338777296</td></tr>\n",
       "<tr><td>2.4116235511053596</td><td>2.038928316809099</td><td>0.09564577831587825</td><td>0.12876640961108698</td></tr>\n",
       "<tr><td>2.3446204414808824</td><td>2.1620985552733023</td><td>-0.0424013188343698</td><td>0.20299660662345526</td></tr>\n",
       "<tr><td>1.5224413917381097</td><td>1.4464455008707857</td><td>0.056652858388635034</td><td>0.1648556391943954</td></tr>\n",
       "<tr><td>2.071907865468317</td><td>1.757569633659934</td><td>0.0721102841575518</td><td>-0.06863285853206544</td></tr>\n",
       "</table></div>"
      ],
      "text/plain": [
       "<Table length=5>\n",
       "      mass1              mass2        ...         spin2z       \n",
       "     float64            float64       ...        float64       \n",
       "------------------ ------------------ ... ---------------------\n",
       "1.6334784673051967 1.3810887548655666 ... -0.012026548338777296\n",
       "2.4116235511053596  2.038928316809099 ...   0.12876640961108698\n",
       "2.3446204414808824 2.1620985552733023 ...   0.20299660662345526\n",
       "1.5224413917381097 1.4464455008707857 ...    0.1648556391943954\n",
       " 2.071907865468317  1.757569633659934 ...  -0.06863285853206544"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "distribution_samples = \"../../scenarios/farah.h5\"\n",
    "samples = Table.read(distribution_samples)[0:5]\n",
    "samples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cacdc51",
   "metadata": {},
   "source": [
    "### 5. Compute the Maximum Redshift (`max_z`) for Each Sample\n",
    "\n",
    "For each sample, we compute the maximum redshift `max_z` at which the SNR exceeds the chosen threshold, using all available PSDs.\n",
    "This approach uses the entire PSD frequency range to ensure accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "80491bbb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "649076c8bfc54d1cafe9de7f1786f4a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:BAYESTAR:Selected template: IMRPhenomD\n",
      "INFO:BAYESTAR:Selected template: IMRPhenomD\n",
      "INFO:BAYESTAR:Selected template: IMRPhenomD\n",
      "INFO:BAYESTAR:Selected template: IMRPhenomD\n",
      "INFO:BAYESTAR:Selected template: IMRPhenomD\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([1.90879562, 3.01082865, 3.05186255, 1.90409028, 2.49130032])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_z = gwcosmo.get_max_z(\n",
    "    psds,\n",
    "    waveform,\n",
    "    f_low,\n",
    "    snr_threshold,\n",
    "    min_triggers,\n",
    "    samples[\"mass1\"],\n",
    "    samples[\"mass2\"],\n",
    "    samples[\"spin1z\"],\n",
    "    samples[\"spin2z\"],\n",
    "    jobs=jobs,\n",
    ")\n",
    "\n",
    "max_z"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57fcd5a6",
   "metadata": {},
   "source": [
    "### 6. Convert `max_z` to Comoving Sensitive Distance\n",
    "\n",
    "We convert each value of `max_z` to a comoving sensitive distance (in Mpc) using the cosmological model.\n",
    "Because `max_z` is a vector, the conversion must be applied to each element separately\n",
    "to avoid errors with array-valued integration.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "f442c909",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3998.333539  , 4707.80752121, 4726.72584292, 3994.16514986,\n",
       "       4429.10918943])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_distance = gwcosmo.sensitive_distance(max_z).to_value(units.Mpc)\n",
    "max_distance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32038c0d",
   "metadata": {},
   "source": [
    "### 7. Compute Detection Volumes and Selection Probabilities\n",
    "\n",
    "- For each sample, use the sensitive distance to calculate the corresponding detection volume:\n",
    "  \n",
    "  $$\n",
    "  V = \\frac{4}{3}\\pi \\times (\\text{distance})^3\n",
    "  $$\n",
    "\n",
    "- These volumes are proportional to each sample's detection probability.\n",
    "- Finally, for each sample, calculate the product `V * T`, where `T` is the observation time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "ed4c45f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# uniform weight per sample, each distance contributes equally at first.\n",
    "probs = 1 / len(max_distance)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab9c8988",
   "metadata": {},
   "source": [
    "### From Equal to Volume-Weighted Probabilities\n",
    "\n",
    "- Instead of giving all distances equal probability, we weight each distance by the amount of observable space (volume) at that distance.\n",
    "\n",
    "- Because sources are uniformly distributed in space, a larger volume means a higher chance of containing a source.\n",
    "- Thus, the probability of detecting a source increases with the volume we can observe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "03cc51b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5.35495302e+10 8.74127333e+10 8.84707777e+10 5.33822235e+10\n",
      " 7.27893374e+10]\n"
     ]
    }
   ],
   "source": [
    "probs *= 4 / 3 * np.pi * max_distance**3\n",
    "print(probs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa65c8a2",
   "metadata": {},
   "source": [
    "### Step: Normalize the Probabilities\n",
    "\n",
    "To obtain a valid probability distribution:\n",
    "\n",
    "- We divide each probability (or volume) by the total sum, so that the sum of all probabilities is exactly 1.\n",
    "\n",
    "- This way, the probabilities represent a proper distribution that can be used to randomly draw synthetic events according to their likelihood.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "d4dea663",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sum of probabilities (before normalization): 355604601947.13995\n",
      "\n",
      "Normalized probabilities:\n",
      " [0.15058728 0.2458144  0.24878974 0.1501168  0.20469178]\n",
      "\n",
      "Sum of normalized probabilities: 1.0\n"
     ]
    }
   ],
   "source": [
    "# Normalize the probabilities so they sum to 1\n",
    "volume_sum = probs.sum()\n",
    "print(\"Sum of probabilities (before normalization):\", volume_sum)\n",
    "\n",
    "probs /= volume_sum  # Element-wise division\n",
    "\n",
    "print(\"\\nNormalized probabilities:\\n\", probs)\n",
    "print(\"\\nSum of normalized probabilities:\", probs.sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5443ef8",
   "metadata": {},
   "source": [
    "#### 8. Simulate events by drawing samples according to their normalized probabilities, using either weighted sampling or a weighted random distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "0dff8546",
   "metadata": {},
   "outputs": [],
   "source": [
    "nsamples = len(samples)\n",
    "dist = stats.rv_discrete(values=(np.arange(len(probs)), probs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "4254b912",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_batches = max(nsamples * len(probs) // 1_000_000_000, 1)\n",
    "batch_sizes = [\n",
    "    len(subarray) for subarray in np.array_split(np.empty(nsamples), n_batches)\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe1a28a4",
   "metadata": {},
   "source": [
    "#### 9. Now, using `dist.rvs()`, we can randomly sample event indices:\n",
    "Indices corresponding to higher probabilities are more likely to be chosen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "bc19ee52",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, 4, 1])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indices = np.concatenate([dist.rvs(size=batch_size) for batch_size in batch_sizes])\n",
    "indices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f68757f5",
   "metadata": {},
   "source": [
    "### 10.  The `cols` dictionary now contains only the selected properties (`mass1`, `mass2`, `spin1z`, `spin2z`) for the sampled events. Each entry is an array of values corresponding to your randomly drawn sample indices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "b06d2fb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:numexpr.utils:NumExpr defaulting to 12 threads.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mass1</th>\n",
       "      <th>mass2</th>\n",
       "      <th>spin1z</th>\n",
       "      <th>spin2z</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2.411624</td>\n",
       "      <td>2.038928</td>\n",
       "      <td>0.095646</td>\n",
       "      <td>0.128766</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.411624</td>\n",
       "      <td>2.038928</td>\n",
       "      <td>0.095646</td>\n",
       "      <td>0.128766</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.411624</td>\n",
       "      <td>2.038928</td>\n",
       "      <td>0.095646</td>\n",
       "      <td>0.128766</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2.071908</td>\n",
       "      <td>1.757570</td>\n",
       "      <td>0.072110</td>\n",
       "      <td>-0.068633</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.411624</td>\n",
       "      <td>2.038928</td>\n",
       "      <td>0.095646</td>\n",
       "      <td>0.128766</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      mass1     mass2    spin1z    spin2z\n",
       "0  2.411624  2.038928  0.095646  0.128766\n",
       "1  2.411624  2.038928  0.095646  0.128766\n",
       "2  2.411624  2.038928  0.095646  0.128766\n",
       "3  2.071908  1.757570  0.072110 -0.068633\n",
       "4  2.411624  2.038928  0.095646  0.128766"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Select columns for the sampled events, keeping only the desired keys\n",
    "import pandas as pd\n",
    "\n",
    "cols = {key: samples[key][indices] for key in [\"mass1\", \"mass2\", \"spin1z\", \"spin2z\"]}\n",
    "pd.DataFrame(cols)  # Display the dictionary of selected columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "169a8413",
   "metadata": {},
   "source": [
    "#### Calculate the Volumetric Rate\n",
    "\n",
    "We estimate the volumetric event rate by dividing the number of simulated events by the total sensitive volume,\n",
    "and express the result in units of events per year per cubic megaparsec (yr⁻¹ Mpc⁻³)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "d5c64b00",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$1.406056 \\times 10^{-11} \\; \\mathrm{\\frac{1}{yr\\,Mpc^{3}}}$"
      ],
      "text/plain": [
       "<Quantity 1.40605604e-11 1 / (yr Mpc3)>"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "volumetric_rate = nsamples / volume_sum * units.year**-1 * units.Mpc**-3\n",
    "volumetric_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "a0ab6167",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2994.45759318, 3469.51154598, 4102.32675857,  734.82961419,\n",
       "       4522.71971105])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Draw random extrinsic parameters\n",
    "distance = stats.powerlaw(a=3, scale=max_distance[indices]).rvs(size=nsamples)\n",
    "distance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1c71093",
   "metadata": {},
   "source": [
    "# Processing with All Simulations\n",
    "\n",
    "After processing all the simulations, we will consider the new observing scenarios based on the GWTC-3 catalog:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea5bc261",
   "metadata": {},
   "source": [
    "## We need to standardize our rate base on the initial distribution rate "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "ee06b04f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><i>Table length=3</i>\n",
       "<table id=\"table7215803408\" class=\"table-striped table-bordered table-condensed\">\n",
       "<thead><tr><th>population</th><th>lower</th><th>mid</th><th>upper</th></tr></thead>\n",
       "<thead><tr><th>str4</th><th>float64</th><th>float64</th><th>float64</th></tr></thead>\n",
       "<tr><td>BNS</td><td>100.0</td><td>240.0</td><td>510.0</td></tr>\n",
       "<tr><td>NSBH</td><td>100.0</td><td>240.0</td><td>510.0</td></tr>\n",
       "<tr><td>BBH</td><td>100.0</td><td>240.0</td><td>510.0</td></tr>\n",
       "</table></div>"
      ],
      "text/plain": [
       "<Table length=3>\n",
       "population  lower    mid    upper \n",
       "   str4    float64 float64 float64\n",
       "---------- ------- ------- -------\n",
       "       BNS   100.0   240.0   510.0\n",
       "      NSBH   100.0   240.0   510.0\n",
       "       BBH   100.0   240.0   510.0"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Lower 5% and upper 95% quantiles of log-normal distribution for different CBC populations\n",
    "run_names = [\"O4\", \"O5\"]\n",
    "rates_table = Table(\n",
    "    [\n",
    "        # Quantiles from O3 R&P paper Table II, row 1, last column\n",
    "        {\"population\": \"BNS\", \"lower\": 100.0, \"mid\": 240.0, \"upper\": 510.0},\n",
    "        {\"population\": \"NSBH\", \"lower\": 100.0, \"mid\": 240.0, \"upper\": 510.0},\n",
    "        {\"population\": \"BBH\", \"lower\": 100.0, \"mid\": 240.0, \"upper\": 510.0},\n",
    "    ]\n",
    ")\n",
    "\n",
    "rates_table"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4320027b",
   "metadata": {},
   "source": [
    "#### Splitting Compact Binary Populations\n",
    "\n",
    "To classify events as BNS (Binary Neutron Star), NSBH (Neutron Star–Black Hole), or BBH (Binary Black Hole),  \n",
    "we use an upper/lower mass limit of 3 solar masses to distinguish neutron stars (NS) from black holes (BH).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "3cc08b58",
   "metadata": {},
   "outputs": [],
   "source": [
    "ns_max_mass = 3.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c8af39f",
   "metadata": {},
   "source": [
    "#### Injection Summary\n",
    "\n",
    "We injected a total of **1 million** compact binary coalescences (CBCs), consisting of:\n",
    "- **892,762** binary neutron stars (BNS)\n",
    "- **35,962** neutron star–black hole binaries (NSBH)\n",
    "- **71,276** binary black holes (BBH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "52353238",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><i>Table length=3</i>\n",
       "<table id=\"table7215803408\" class=\"table-striped table-bordered table-condensed\">\n",
       "<thead><tr><th>population</th><th>lower</th><th>mid</th><th>upper</th><th>mass_fraction</th></tr></thead>\n",
       "<thead><tr><th>str4</th><th>float64</th><th>float64</th><th>float64</th><th>float64</th></tr></thead>\n",
       "<tr><td>BNS</td><td>100.0</td><td>240.0</td><td>510.0</td><td>0.892762</td></tr>\n",
       "<tr><td>NSBH</td><td>100.0</td><td>240.0</td><td>510.0</td><td>0.035962</td></tr>\n",
       "<tr><td>BBH</td><td>100.0</td><td>240.0</td><td>510.0</td><td>0.071276</td></tr>\n",
       "</table></div>"
      ],
      "text/plain": [
       "<Table length=3>\n",
       "population  lower    mid    upper  mass_fraction\n",
       "   str4    float64 float64 float64    float64   \n",
       "---------- ------- ------- ------- -------------\n",
       "       BNS   100.0   240.0   510.0      0.892762\n",
       "      NSBH   100.0   240.0   510.0      0.035962\n",
       "       BBH   100.0   240.0   510.0      0.071276"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate the mass fraction for each CBC population (BNS, NSBH, BBH)\n",
    "# by dividing the population count by the total number of CBCs (1 million)\n",
    "rates_table[\"mass_fraction\"] = (\n",
    "    np.array([892762, 35962, 71276]) / 1e6\n",
    ")  # Total CBCs = 1 million\n",
    "\n",
    "\n",
    "rates_table"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2207cad",
   "metadata": {},
   "source": [
    "##### === Simulated BNS Merger Rates ===\n",
    "In observing scenarios data there is a file sqlite  file \"sqlite3 events.sqlite\" where we can read the \n",
    "the simulation Merger rate, for that use this command line in terminal or import sqlite with python \n",
    "\n",
    " Use this command to retrieve comments:\n",
    "\n",
    " 1- $ sqlite3 events.sqlite\n",
    "\n",
    " 2- $ select comment from process;\n",
    "\n",
    "For example: The simulated CBC merger rate in yr^-1 Gpc^-3 for O5 and O6-HLVK configuration with (SNR = 10)\n",
    "\n",
    "From kiendrebeogo et al. 2023 the simulated rate is given by the by (yr^-1 Mpc^-3)\n",
    "\n",
    "so this need to be convert in yr^-1 Gpc^-3, before add use it here.\n",
    "\n",
    "simulation rate  sim_rate =2.712359951521142e3 (u.Gpc**-3 * u.yr**-1) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ee4c6ba",
   "metadata": {},
   "source": [
    "### === Simulated BNS Merger Rates ===\n",
    "\n",
    "In the observing scenario data, there is an SQLite file called `events.sqlite` containing the simulated merger rates.\n",
    "\n",
    "You can access the merger rate comments using either the command line or Python:\n",
    "\n",
    "**Command-line (SQLite):**\n",
    "1. Open the database:\n",
    "\n",
    "    $ sqlite3 events.sqlite\n",
    "\n",
    "2. Retrieve the merger rate comments:\n",
    "\n",
    "    sqlite> select comment from process;\n",
    "\n",
    "\n",
    "For example: The simulated CBC merger rate in yr^-1 Gpc^-3 for O5 and O6-HLVK configuration with (SNR = 10)\n",
    "\n",
    "From kiendrebeogo et al. 2023 the simulated rate is given by the by (yr^-1 Mpc^-3)\n",
    "\n",
    "so this need to be convert in yr^-1 Gpc^-3, before add use it here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "70264341",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<Column name='sim_rate_O4' dtype='float64' unit='1 / (yr Gpc3)' length=3>\n",
       " 6546.207600000001\n",
       " 6546.207600000001\n",
       " 6546.207600000001,\n",
       " <Column name='sim_rate_O5' dtype='float64' unit='1 / (yr Gpc3)' length=3>\n",
       " 2712.35995\n",
       " 2712.35995\n",
       " 2712.35995)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Here BNS, BBH and NSBH have the same simulation rates as they have been simulated together.\n",
    "rates_table[\"sim_rate_O4\"] = [6.5462076e-06, 6.5462076e-06, 6.5462076e-06] * (\n",
    "    1 / (u.Mpc**3 * u.yr)\n",
    ")\n",
    "rates_table[\"sim_rate_O5\"] = [2.71235995e-06, 2.71235995e-06, 2.71235995e-06] * (\n",
    "    1 / (u.Mpc**3 * u.yr)\n",
    ")\n",
    "\n",
    "# Conversion in Gpc^-3/ yr\n",
    "rates_table[\"sim_rate_O4\"] = rates_table[\"sim_rate_O4\"].to(u.Gpc**-3 * u.yr**-1)\n",
    "rates_table[\"sim_rate_O5\"] = rates_table[\"sim_rate_O5\"].to(u.Gpc**-3 * u.yr**-1)\n",
    "\n",
    "rates_table[\"sim_rate_O4\"], rates_table[\"sim_rate_O5\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37996f0c",
   "metadata": {},
   "source": [
    "# Add the expected number of detections for O4 and O5\n",
    "\n",
    "Here the SNR threshold to confirm a detection is 8.\n",
    "\n",
    "So all those event have a signal noise ratio > 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "38806733",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><i>Table length=3</i>\n",
       "<table id=\"table7215803408\" class=\"table-striped table-bordered table-condensed\">\n",
       "<thead><tr><th>population</th><th>lower</th><th>mid</th><th>upper</th><th>mass_fraction</th><th>sim_rate_O4</th><th>sim_rate_O5</th><th>detection_number_O4</th><th>detection_number_O5</th></tr></thead>\n",
       "<thead><tr><th></th><th></th><th></th><th></th><th></th><th>1 / (yr Gpc3)</th><th>1 / (yr Gpc3)</th><th></th><th></th></tr></thead>\n",
       "<thead><tr><th>str4</th><th>float64</th><th>float64</th><th>float64</th><th>float64</th><th>float64</th><th>float64</th><th>int64</th><th>int64</th></tr></thead>\n",
       "<tr><td>BNS</td><td>100.0</td><td>240.0</td><td>510.0</td><td>0.892762</td><td>6546.207600000001</td><td>2712.35995</td><td>1004</td><td>2003</td></tr>\n",
       "<tr><td>NSBH</td><td>100.0</td><td>240.0</td><td>510.0</td><td>0.035962</td><td>6546.207600000001</td><td>2712.35995</td><td>184</td><td>356</td></tr>\n",
       "<tr><td>BBH</td><td>100.0</td><td>240.0</td><td>510.0</td><td>0.071276</td><td>6546.207600000001</td><td>2712.35995</td><td>7070</td><td>9809</td></tr>\n",
       "</table></div>"
      ],
      "text/plain": [
       "<Table length=3>\n",
       "population  lower    mid   ... detection_number_O4 detection_number_O5\n",
       "                           ...                                        \n",
       "   str4    float64 float64 ...        int64               int64       \n",
       "---------- ------- ------- ... ------------------- -------------------\n",
       "       BNS   100.0   240.0 ...                1004                2003\n",
       "      NSBH   100.0   240.0 ...                 184                 356\n",
       "       BBH   100.0   240.0 ...                7070                9809"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rates_table[\"detection_number_O4\"] = np.array([1004, 184, 7070])\n",
    "rates_table[\"detection_number_O5\"] = np.array([2003, 356, 9809])\n",
    "\n",
    "rates_table"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "444aec20",
   "metadata": {},
   "source": [
    "# Scaling Quantiles by Mass Fraction\n",
    "\n",
    "For each CBC population, we scale the lower, median, and upper event rate quantiles by `mass_fraction` column. \n",
    "\n",
    "This can be used, for example, to focus on a sub-population or a fraction of events relevant to a specific analysis.\n",
    "\n",
    "> **Note:** Make sure that the `mass_fraction` column exists and has appropriate values (between 0 and 1).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "0f806a10",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><i>Table length=3</i>\n",
       "<table id=\"table7215803408\" class=\"table-striped table-bordered table-condensed\">\n",
       "<thead><tr><th>population</th><th>lower</th><th>mid</th><th>upper</th><th>mass_fraction</th><th>sim_rate_O4</th><th>sim_rate_O5</th><th>detection_number_O4</th><th>detection_number_O5</th></tr></thead>\n",
       "<thead><tr><th></th><th></th><th></th><th></th><th></th><th>1 / (yr Gpc3)</th><th>1 / (yr Gpc3)</th><th></th><th></th></tr></thead>\n",
       "<thead><tr><th>str4</th><th>float64</th><th>float64</th><th>float64</th><th>float64</th><th>float64</th><th>float64</th><th>int64</th><th>int64</th></tr></thead>\n",
       "<tr><td>BNS</td><td>89.27619999999999</td><td>214.26288</td><td>455.30861999999996</td><td>0.892762</td><td>6546.207600000001</td><td>2712.35995</td><td>1004</td><td>2003</td></tr>\n",
       "<tr><td>NSBH</td><td>3.5962</td><td>8.63088</td><td>18.34062</td><td>0.035962</td><td>6546.207600000001</td><td>2712.35995</td><td>184</td><td>356</td></tr>\n",
       "<tr><td>BBH</td><td>7.127600000000001</td><td>17.10624</td><td>36.35076</td><td>0.071276</td><td>6546.207600000001</td><td>2712.35995</td><td>7070</td><td>9809</td></tr>\n",
       "</table></div>"
      ],
      "text/plain": [
       "<Table length=3>\n",
       "population       lower       ... detection_number_O4 detection_number_O5\n",
       "                             ...                                        \n",
       "   str4         float64      ...        int64               int64       \n",
       "---------- ----------------- ... ------------------- -------------------\n",
       "       BNS 89.27619999999999 ...                1004                2003\n",
       "      NSBH            3.5962 ...                 184                 356\n",
       "       BBH 7.127600000000001 ...                7070                9809"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for key in [\"lower\", \"mid\", \"upper\"]:\n",
    "    rates_table[key] *= rates_table[\"mass_fraction\"]\n",
    "\n",
    "rates_table"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d647abd",
   "metadata": {},
   "source": [
    "## Compute Log-normal Parameters from Quantiles\n",
    "\n",
    "For each population, we calculate the mean (`mu`) and standard deviation (`sigma`) of the underlying log-normal distribution.\n",
    "- `mu` is the mean of the logarithm of the median event rate.\n",
    "- `sigma` is set so that the interval between the lower and upper quantile matches the standard width of a 90% normal interval (from 5% to 95% quantile).\n",
    "\n",
    "This is useful for statistical modeling or simulations where log-normal priors are assumed for event rates.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "ab1be018",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><i>Table length=3</i>\n",
       "<table id=\"table7215803408\" class=\"table-striped table-bordered table-condensed\">\n",
       "<thead><tr><th>population</th><th>lower</th><th>mid</th><th>upper</th><th>mass_fraction</th><th>sim_rate_O4</th><th>sim_rate_O5</th><th>detection_number_O4</th><th>detection_number_O5</th><th>mu</th><th>sigma</th></tr></thead>\n",
       "<thead><tr><th></th><th></th><th></th><th></th><th></th><th>1 / (yr Gpc3)</th><th>1 / (yr Gpc3)</th><th></th><th></th><th></th><th></th></tr></thead>\n",
       "<thead><tr><th>str4</th><th>float64</th><th>float64</th><th>float64</th><th>float64</th><th>float64</th><th>float64</th><th>int64</th><th>int64</th><th>float64</th><th>float64</th></tr></thead>\n",
       "<tr><td>BNS</td><td>89.27619999999999</td><td>214.26288</td><td>455.30861999999996</td><td>0.892762</td><td>6546.207600000001</td><td>2712.35995</td><td>1004</td><td>2003</td><td>5.367203672357068</td><td>0.49525395847832065</td></tr>\n",
       "<tr><td>NSBH</td><td>3.5962</td><td>8.63088</td><td>18.34062</td><td>0.035962</td><td>6546.207600000001</td><td>2712.35995</td><td>184</td><td>356</td><td>2.1553464697693</td><td>0.49525395847832093</td></tr>\n",
       "<tr><td>BBH</td><td>7.127600000000001</td><td>17.10624</td><td>36.35076</td><td>0.071276</td><td>6546.207600000001</td><td>2712.35995</td><td>7070</td><td>9809</td><td>2.8394433092250226</td><td>0.4952539584783208</td></tr>\n",
       "</table></div>"
      ],
      "text/plain": [
       "<Table length=3>\n",
       "population       lower       ...         mu                sigma       \n",
       "                             ...                                       \n",
       "   str4         float64      ...      float64             float64      \n",
       "---------- ----------------- ... ------------------ -------------------\n",
       "       BNS 89.27619999999999 ...  5.367203672357068 0.49525395847832065\n",
       "      NSBH            3.5962 ...    2.1553464697693 0.49525395847832093\n",
       "       BBH 7.127600000000001 ... 2.8394433092250226  0.4952539584783208"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(standard_90pct_interval,) = np.diff(stats.norm.interval(0.9))\n",
    "rates_table[\"mu\"] = np.log(rates_table[\"mid\"])\n",
    "rates_table[\"sigma\"] = (\n",
    "    np.log(rates_table[\"upper\"]) - np.log(rates_table[\"lower\"])\n",
    ") / standard_90pct_interval\n",
    "\n",
    "rates_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "da0c49ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([5.36720367, 2.15534647, 2.83944331]),\n",
       " array([0.49525396, 0.49525396, 0.49525396]))"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Extract the log-normal parameters for each population as numpy arrays\n",
    "fiducial_log_rates = np.asarray(rates_table[\"mu\"])\n",
    "fiducial_log_rate_errs = np.asarray(rates_table[\"sigma\"])\n",
    "\n",
    "fiducial_log_rates, fiducial_log_rate_errs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05600254",
   "metadata": {},
   "source": [
    "### Add Log-normal Parameters to the Table\n",
    "\n",
    "For convenience, we add the log-normal mean (`fiducial_log_rate`) and standard deviation (`fiducial_log_rate_err`) as new columns in the `rates_table`.  \n",
    "This keeps all relevant parameters together and makes the table easy to use for further analysis or plotting.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "3733dbf5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><i>Table length=3</i>\n",
       "<table id=\"table7215803408\" class=\"table-striped table-bordered table-condensed\">\n",
       "<thead><tr><th>population</th><th>lower</th><th>mid</th><th>upper</th><th>mass_fraction</th><th>sim_rate_O4</th><th>sim_rate_O5</th><th>detection_number_O4</th><th>detection_number_O5</th><th>mu</th><th>sigma</th><th>fiducial_log_rate</th><th>fiducial_log_rate_err</th></tr></thead>\n",
       "<thead><tr><th></th><th></th><th></th><th></th><th></th><th>1 / (yr Gpc3)</th><th>1 / (yr Gpc3)</th><th></th><th></th><th></th><th></th><th></th><th></th></tr></thead>\n",
       "<thead><tr><th>str4</th><th>float64</th><th>float64</th><th>float64</th><th>float64</th><th>float64</th><th>float64</th><th>int64</th><th>int64</th><th>float64</th><th>float64</th><th>float64</th><th>float64</th></tr></thead>\n",
       "<tr><td>BNS</td><td>89.27619999999999</td><td>214.26288</td><td>455.30861999999996</td><td>0.892762</td><td>6546.207600000001</td><td>2712.35995</td><td>1004</td><td>2003</td><td>5.367203672357068</td><td>0.49525395847832065</td><td>5.367203672357068</td><td>0.49525395847832065</td></tr>\n",
       "<tr><td>NSBH</td><td>3.5962</td><td>8.63088</td><td>18.34062</td><td>0.035962</td><td>6546.207600000001</td><td>2712.35995</td><td>184</td><td>356</td><td>2.1553464697693</td><td>0.49525395847832093</td><td>2.1553464697693</td><td>0.49525395847832093</td></tr>\n",
       "<tr><td>BBH</td><td>7.127600000000001</td><td>17.10624</td><td>36.35076</td><td>0.071276</td><td>6546.207600000001</td><td>2712.35995</td><td>7070</td><td>9809</td><td>2.8394433092250226</td><td>0.4952539584783208</td><td>2.8394433092250226</td><td>0.4952539584783208</td></tr>\n",
       "</table></div>"
      ],
      "text/plain": [
       "<Table length=3>\n",
       "population       lower       ... fiducial_log_rate  fiducial_log_rate_err\n",
       "                             ...                                         \n",
       "   str4         float64      ...      float64              float64       \n",
       "---------- ----------------- ... ------------------ ---------------------\n",
       "       BNS 89.27619999999999 ...  5.367203672357068   0.49525395847832065\n",
       "      NSBH            3.5962 ...    2.1553464697693   0.49525395847832093\n",
       "       BBH 7.127600000000001 ... 2.8394433092250226    0.4952539584783208"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rates_table[\"fiducial_log_rate\"] = fiducial_log_rates\n",
    "rates_table[\"fiducial_log_rate_err\"] = fiducial_log_rate_errs\n",
    "\n",
    "rates_table"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56a97eea",
   "metadata": {},
   "source": [
    "#### Functions for propagating errors in rates\n",
    "\n",
    "Reproduced from https://github.com/lpsinger/observing-scenarios-simulations/blob/main/plots-and-tables.ipynb."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "36c0c995",
   "metadata": {},
   "outputs": [],
   "source": [
    "def betabinom_k_n(k, n):\n",
    "    return stats.betabinom(n, k + 1, n - k + 1)\n",
    "\n",
    "\n",
    "@np.vectorize\n",
    "def poisson_lognormal_rate_cdf(k, mu, sigma):\n",
    "    lognorm_pdf = stats.lognorm(s=sigma, scale=np.exp(mu)).pdf\n",
    "\n",
    "    def func(lam):\n",
    "        prior = lognorm_pdf(lam)\n",
    "        # poisson_pdf = np.exp(special.xlogy(k, lam) - special.gammaln(k + 1) - lam)\n",
    "        poisson_cdf = special.gammaincc(k + 1, lam)\n",
    "        return poisson_cdf * prior\n",
    "\n",
    "    # Marginalize over lambda.\n",
    "    #\n",
    "    # Note that we use scipy.integrate.odeint instead\n",
    "    # of scipy.integrate.quad because it is important for the stability of\n",
    "    # root_scalar below that we calculate the pdf and the cdf at the same time,\n",
    "    # using the same exact quadrature rule.\n",
    "    cdf, _ = integrate.quad(func, 0, np.inf, epsabs=0)\n",
    "    return cdf\n",
    "\n",
    "\n",
    "@np.vectorize\n",
    "def poisson_lognormal_rate_quantiles(p, mu, sigma):\n",
    "    \"\"\"Find the quantiles of a Poisson distribution with\n",
    "    a log-normal prior on its rate.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    p : float\n",
    "        The quantiles at which to find the number of counts.\n",
    "    mu : float\n",
    "        The mean of the log of the rate.\n",
    "    sigma : float\n",
    "        The standard deviation of the log of the rate.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    k : float\n",
    "        The number of events.\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    This algorithm treats the Poisson count k as a continuous\n",
    "    real variable so that it can use the scipy.optimize.root_scalar\n",
    "    root finding/polishing algorithms.\n",
    "    \"\"\"\n",
    "\n",
    "    def func(k):\n",
    "        return poisson_lognormal_rate_cdf(k, mu, sigma) - p\n",
    "\n",
    "    if func(0) >= 0:\n",
    "        return 0\n",
    "\n",
    "    result = optimize.root_scalar(func, bracket=[0, 1e6])\n",
    "    return result.root\n",
    "\n",
    "\n",
    "def format_with_errorbars(mid, lo, hi):\n",
    "    plus = hi - mid\n",
    "    minus = mid - lo\n",
    "    smallest = min(max(0, plus), max(0, minus))\n",
    "\n",
    "    if smallest == 0:\n",
    "        return str(mid), \"0\", \"0\"\n",
    "    decimals = 1 - int(np.floor(np.log10(smallest)))\n",
    "\n",
    "    if all(np.issubdtype(type(_), np.integer) for _ in (mid, lo, hi)):\n",
    "        decimals = min(decimals, 0)\n",
    "\n",
    "    plus, minus, mid = np.round([plus, minus, mid], decimals)\n",
    "    if decimals > 0:\n",
    "        fstring = \"%%.0%df\" % decimals\n",
    "    else:\n",
    "        fstring = \"%d\"\n",
    "    return [fstring % _ for _ in [mid, minus, plus]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d20ce050",
   "metadata": {},
   "source": [
    "### Set Probability Quantiles and Run Duration\n",
    "\n",
    "- `prob_quantiles` defines the lower (5%), median (50%), and upper (95%) quantiles used for statistical calculations.\n",
    "- `run_duration` is the duration of the observing run in years.  \n",
    "You can adjust this value if you need rates for a different observation period.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "bad0e45e",
   "metadata": {},
   "outputs": [],
   "source": [
    "prob_quantiles = np.asarray([0.05, 0.5, 0.95])\n",
    "run_duration = 1.0  # years"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0dc551a",
   "metadata": {},
   "source": [
    "### Detection Number Quantiles Table\n",
    "\n",
    "This block computes the 5%, 50%, and 95% quantiles for the expected number of detections for each CBC population and run, based on log-normal statistics.\n",
    "\n",
    "**Table columns:**\n",
    "- `run`: Observing run (e.g., O4, O5)\n",
    "- `population`: Source type (BNS, NSBH, BBH)\n",
    "- `lo`, `mid`, `hi`: Lower, median, and upper quantiles for the expected number of detections\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "d97dfcfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14.220903001694502 36.29137411178602 84.26422037747086\n",
      "1.1678211882111724 6.178323663302014 16.280011220976373\n",
      "112.62898605723846 258.7007704065045 586.5212332465501\n",
      "76.33752375313456 176.72913138564286 401.40447798110256\n",
      "11.882852221599126 30.979862729339093 72.27134606773731\n",
      "382.1724595582784 867.437073084928 1961.2393670952767\n"
     ]
    }
   ],
   "source": [
    "results = {}\n",
    "for run_name in run_names:\n",
    "    results[run_name] = {}\n",
    "\n",
    "    for pop in [\"BNS\", \"NSBH\", \"BBH\"]:\n",
    "        rates_row = rates_table[rates_table[\"population\"] == pop]\n",
    "        rate = rates_row[f\"sim_rate_{run_name}\"] * rates_row[\"mass_fraction\"]\n",
    "\n",
    "        mu = (\n",
    "            rates_row[\"fiducial_log_rate\"]\n",
    "            + np.log(run_duration)\n",
    "            + np.log(rates_row[f\"detection_number_{run_name}\"] / rate)\n",
    "        )\n",
    "        sigma = rates_row[\"fiducial_log_rate_err\"]\n",
    "\n",
    "        lo, mid, hi = poisson_lognormal_rate_quantiles(prob_quantiles, mu, sigma)\n",
    "        print(lo, mid, hi)\n",
    "        lo = int(np.floor(lo))\n",
    "        mid = int(np.round(mid))\n",
    "        hi = int(np.ceil(hi))\n",
    "\n",
    "        mid, lo, hi = format_with_errorbars(mid, lo, hi)\n",
    "\n",
    "        results[run_name].setdefault(\"low\", {})[pop] = lo\n",
    "        results[run_name].setdefault(\"mid\", {})[pop] = mid\n",
    "        results[run_name].setdefault(\"high\", {})[pop] = hi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "4141b82a",
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = []\n",
    "for run_name, run_stats in results.items():\n",
    "    for pop in run_stats[\"mid\"]:\n",
    "        rows.append(\n",
    "            {\n",
    "                \"run\": run_name,\n",
    "                \"population\": pop,\n",
    "                \"low\": f\"-{run_stats['low'][pop]}\",\n",
    "                \"mid\": run_stats[\"mid\"][pop],\n",
    "                \"high\": f\"+{run_stats['high'][pop]}\",\n",
    "            }\n",
    "        )\n",
    "\n",
    "results_table = Table(rows=rows, names=(\"run\", \"population\", \"low\", \"mid\", \"high\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "19755481",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><i>Table length=6</i>\n",
       "<table id=\"table6932093520\" class=\"table-striped table-bordered table-condensed\">\n",
       "<thead><tr><th>run</th><th>population</th><th>low</th><th>mid</th><th>high</th></tr></thead>\n",
       "<thead><tr><th>str2</th><th>str4</th><th>str4</th><th>str3</th><th>str5</th></tr></thead>\n",
       "<tr><td>O4</td><td>BNS</td><td>-22</td><td>36</td><td>+49</td></tr>\n",
       "<tr><td>O4</td><td>NSBH</td><td>-5</td><td>6</td><td>+11</td></tr>\n",
       "<tr><td>O4</td><td>BBH</td><td>-150</td><td>260</td><td>+330</td></tr>\n",
       "<tr><td>O5</td><td>BNS</td><td>-100</td><td>180</td><td>+220</td></tr>\n",
       "<tr><td>O5</td><td>NSBH</td><td>-20</td><td>31</td><td>+42</td></tr>\n",
       "<tr><td>O5</td><td>BBH</td><td>-480</td><td>870</td><td>+1100</td></tr>\n",
       "</table></div>"
      ],
      "text/plain": [
       "<Table length=6>\n",
       "run  population low  mid   high\n",
       "str2    str4    str4 str3  str5\n",
       "---- ---------- ---- ---- -----\n",
       "  O4        BNS  -22   36   +49\n",
       "  O4       NSBH   -5    6   +11\n",
       "  O4        BBH -150  260  +330\n",
       "  O5        BNS -100  180  +220\n",
       "  O5       NSBH  -20   31   +42\n",
       "  O5        BBH -480  870 +1100"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "ba1677d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Observing Run O4 : Annual number of detections\n",
      "===============================================\n",
      "run population low  mid high\n",
      "--- ---------- ---- --- ----\n",
      " O4        BNS  -22  36  +49\n",
      " O4       NSBH   -5   6  +11\n",
      " O4        BBH -150 260 +330\n",
      "===============================================\n",
      "\n",
      " Observing Run O5 : Annual number of detections\n",
      "===============================================\n",
      "run population low  mid  high\n",
      "--- ---------- ---- --- -----\n",
      " O5        BNS -100 180  +220\n",
      " O5       NSBH  -20  31   +42\n",
      " O5        BBH -480 870 +1100\n",
      "===============================================\n"
     ]
    }
   ],
   "source": [
    "# display by run\n",
    "for run_name in sorted(set(results_table[\"run\"])):\n",
    "    print(f\"\\n Observing Run {run_name} : Annual number of detections\")\n",
    "    print(\"=\" * 47)\n",
    "    print(results_table[results_table[\"run\"] == run_name])\n",
    "    print(\"=\" * 47)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1753f9f7",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fc113583",
   "metadata": {},
   "source": [
    "# For Petrov et al. 2O22 , design as LRR distribustion, \n",
    "\n",
    "the BNS , NSBH and BBH as consider as a population not a sub-pospulation from CBC ,\n",
    "\n",
    "In fact we simulate each population independently to the other ones, so 1 million each.\n",
    "\n",
    "This means there is no longer a mass fraction or simply means the mass fraction is 1 because as we simulate each population at one time.\n",
    "This"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "a777824f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><i>Table length=3</i>\n",
       "<table id=\"table7215964432\" class=\"table-striped table-bordered table-condensed\">\n",
       "<thead><tr><th>population</th><th>lower</th><th>mid</th><th>upper</th></tr></thead>\n",
       "<thead><tr><th>str4</th><th>float64</th><th>float64</th><th>float64</th></tr></thead>\n",
       "<tr><td>BNS</td><td>80.0</td><td>320.0</td><td>810.0</td></tr>\n",
       "<tr><td>NSBH</td><td>61.0</td><td>130.0</td><td>242.0</td></tr>\n",
       "<tr><td>BBH</td><td>15.3</td><td>23.9</td><td>38.2</td></tr>\n",
       "</table></div>"
      ],
      "text/plain": [
       "<Table length=3>\n",
       "population  lower    mid    upper \n",
       "   str4    float64 float64 float64\n",
       "---------- ------- ------- -------\n",
       "       BNS    80.0   320.0   810.0\n",
       "      NSBH    61.0   130.0   242.0\n",
       "       BBH    15.3    23.9    38.2"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Lower 5% and upper 95% quantiles of log-normal distribution for different CBC populations\n",
    "run_names = [\"O4\", \"O5\"]\n",
    "rates_table = Table(\n",
    "    [\n",
    "        # BNS rate from GWTC-2\n",
    "        # https://doi.org/10.3847/2041-8213/abe949\n",
    "        {\"population\": \"BNS\", \"lower\": 80.00, \"mid\": 320.0, \"upper\": 810.0},\n",
    "        # NSBH rate from GW200105 and GW200115 paper\n",
    "        # https://doi.org/10.3847/2041-8213/ac082e\n",
    "        {\"population\": \"NSBH\", \"lower\": 61.0, \"mid\": 130.0, \"upper\": 242.0},\n",
    "        # BBH rate from GWTC-2\n",
    "        # https://doi.org/10.3847/2041-8213/abe949\n",
    "        {\"population\": \"BBH\", \"lower\": 15.3, \"mid\": 23.9, \"upper\": 38.2},\n",
    "    ]\n",
    ")\n",
    "\n",
    "rates_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "6f41ab0b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><i>Table length=3</i>\n",
       "<table id=\"table7215964432\" class=\"table-striped table-bordered table-condensed\">\n",
       "<thead><tr><th>population</th><th>lower</th><th>mid</th><th>upper</th><th>detection_number_O4</th><th>detection_number_O5</th></tr></thead>\n",
       "<thead><tr><th>str4</th><th>float64</th><th>float64</th><th>float64</th><th>int64</th><th>int64</th></tr></thead>\n",
       "<tr><td>BNS</td><td>80.0</td><td>320.0</td><td>810.0</td><td>1482</td><td>2307</td></tr>\n",
       "<tr><td>NSBH</td><td>61.0</td><td>130.0</td><td>242.0</td><td>2492</td><td>5441</td></tr>\n",
       "<tr><td>BBH</td><td>15.3</td><td>23.9</td><td>38.2</td><td>6040</td><td>21559</td></tr>\n",
       "</table></div>"
      ],
      "text/plain": [
       "<Table length=3>\n",
       "population  lower    mid    upper  detection_number_O4 detection_number_O5\n",
       "   str4    float64 float64 float64        int64               int64       \n",
       "---------- ------- ------- ------- ------------------- -------------------\n",
       "       BNS    80.0   320.0   810.0                1482                2307\n",
       "      NSBH    61.0   130.0   242.0                2492                5441\n",
       "       BBH    15.3    23.9    38.2                6040               21559"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rates_table[\"detection_number_O4\"] = np.array([1482, 2492, 6040])\n",
    "rates_table[\"detection_number_O5\"] = np.array([2307, 5441, 21559])\n",
    "\n",
    "rates_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "21609940",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<Column name='sim_rate_O4' dtype='float64' unit='1 / (yr Gpc3)' length=3>\n",
       " 13598.513465134649\n",
       "  4462.124683568845\n",
       " 1355.9875568260693,\n",
       " <Column name='sim_rate_O5' dtype='float64' unit='1 / (yr Gpc3)' length=3>\n",
       "  3908.209488939609\n",
       " 1952.4639435773054\n",
       " 1080.6507866022996)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rates_table[\"sim_rate_O4\"] = [\n",
    "    1.3598513465134647e-05,\n",
    "    4.462124683568844e-06,\n",
    "    1.355987556826069e-06,\n",
    "] * (1 / (u.Mpc**3 * u.yr))\n",
    "rates_table[\"sim_rate_O5\"] = [\n",
    "    3.908209488939608e-06,\n",
    "    1.952463943577305e-06,\n",
    "    1.0806507866022996e-06,\n",
    "] * (1 / (u.Mpc**3 * u.yr))\n",
    "\n",
    "\n",
    "# Conversion in Gpc^-3/ yr\n",
    "rates_table[\"sim_rate_O4\"] = rates_table[\"sim_rate_O4\"].to(u.Gpc**-3 * u.yr**-1)\n",
    "rates_table[\"sim_rate_O5\"] = rates_table[\"sim_rate_O5\"].to(u.Gpc**-3 * u.yr**-1)\n",
    "\n",
    "\n",
    "rates_table[\"sim_rate_O4\"], rates_table[\"sim_rate_O5\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "93d266c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><i>Table length=3</i>\n",
       "<table id=\"table7215964432\" class=\"table-striped table-bordered table-condensed\">\n",
       "<thead><tr><th>population</th><th>lower</th><th>mid</th><th>upper</th><th>detection_number_O4</th><th>detection_number_O5</th><th>sim_rate_O4</th><th>sim_rate_O5</th><th>mu</th><th>sigma</th></tr></thead>\n",
       "<thead><tr><th></th><th></th><th></th><th></th><th></th><th></th><th>1 / (yr Gpc3)</th><th>1 / (yr Gpc3)</th><th></th><th></th></tr></thead>\n",
       "<thead><tr><th>str4</th><th>float64</th><th>float64</th><th>float64</th><th>int64</th><th>int64</th><th>float64</th><th>float64</th><th>float64</th><th>float64</th></tr></thead>\n",
       "<tr><td>BNS</td><td>80.0</td><td>320.0</td><td>810.0</td><td>1482</td><td>2307</td><td>13598.513465134649</td><td>3908.209488939609</td><td>5.768320995793772</td><td>0.7037123471233048</td></tr>\n",
       "<tr><td>NSBH</td><td>61.0</td><td>130.0</td><td>242.0</td><td>2492</td><td>5441</td><td>4462.124683568845</td><td>1952.4639435773054</td><td>4.867534450455582</td><td>0.41890166985175514</td></tr>\n",
       "<tr><td>BBH</td><td>15.3</td><td>23.9</td><td>38.2</td><td>6040</td><td>21559</td><td>1355.9875568260693</td><td>1080.6507866022996</td><td>3.173878458937465</td><td>0.2781349878864127</td></tr>\n",
       "</table></div>"
      ],
      "text/plain": [
       "<Table length=3>\n",
       "population  lower    mid   ...         mu               sigma       \n",
       "                           ...                                      \n",
       "   str4    float64 float64 ...      float64            float64      \n",
       "---------- ------- ------- ... ----------------- -------------------\n",
       "       BNS    80.0   320.0 ... 5.768320995793772  0.7037123471233048\n",
       "      NSBH    61.0   130.0 ... 4.867534450455582 0.41890166985175514\n",
       "       BBH    15.3    23.9 ... 3.173878458937465  0.2781349878864127"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(standard_90pct_interval,) = np.diff(stats.norm.interval(0.9))\n",
    "rates_table[\"mu\"] = np.log(rates_table[\"mid\"])\n",
    "rates_table[\"sigma\"] = (\n",
    "    np.log(rates_table[\"upper\"]) - np.log(rates_table[\"lower\"])\n",
    ") / standard_90pct_interval\n",
    "\n",
    "rates_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "2c47ff81",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([5.768321  , 4.86753445, 3.17387846]),\n",
       " array([0.70371235, 0.41890167, 0.27813499]))"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Extract the log-normal parameters for each population as numpy arrays\n",
    "fiducial_log_rates = np.asarray(rates_table[\"mu\"])\n",
    "fiducial_log_rate_errs = np.asarray(rates_table[\"sigma\"])\n",
    "\n",
    "fiducial_log_rates, fiducial_log_rate_errs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "7e19d522",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><i>Table length=3</i>\n",
       "<table id=\"table7215964432\" class=\"table-striped table-bordered table-condensed\">\n",
       "<thead><tr><th>population</th><th>lower</th><th>mid</th><th>upper</th><th>detection_number_O4</th><th>detection_number_O5</th><th>sim_rate_O4</th><th>sim_rate_O5</th><th>mu</th><th>sigma</th><th>fiducial_log_rate</th><th>fiducial_log_rate_err</th></tr></thead>\n",
       "<thead><tr><th></th><th></th><th></th><th></th><th></th><th></th><th>1 / (yr Gpc3)</th><th>1 / (yr Gpc3)</th><th></th><th></th><th></th><th></th></tr></thead>\n",
       "<thead><tr><th>str4</th><th>float64</th><th>float64</th><th>float64</th><th>int64</th><th>int64</th><th>float64</th><th>float64</th><th>float64</th><th>float64</th><th>float64</th><th>float64</th></tr></thead>\n",
       "<tr><td>BNS</td><td>80.0</td><td>320.0</td><td>810.0</td><td>1482</td><td>2307</td><td>13598.513465134649</td><td>3908.209488939609</td><td>5.768320995793772</td><td>0.7037123471233048</td><td>5.768320995793772</td><td>0.7037123471233048</td></tr>\n",
       "<tr><td>NSBH</td><td>61.0</td><td>130.0</td><td>242.0</td><td>2492</td><td>5441</td><td>4462.124683568845</td><td>1952.4639435773054</td><td>4.867534450455582</td><td>0.41890166985175514</td><td>4.867534450455582</td><td>0.41890166985175514</td></tr>\n",
       "<tr><td>BBH</td><td>15.3</td><td>23.9</td><td>38.2</td><td>6040</td><td>21559</td><td>1355.9875568260693</td><td>1080.6507866022996</td><td>3.173878458937465</td><td>0.2781349878864127</td><td>3.173878458937465</td><td>0.2781349878864127</td></tr>\n",
       "</table></div>"
      ],
      "text/plain": [
       "<Table length=3>\n",
       "population  lower    mid   ... fiducial_log_rate fiducial_log_rate_err\n",
       "                           ...                                        \n",
       "   str4    float64 float64 ...      float64             float64       \n",
       "---------- ------- ------- ... ----------------- ---------------------\n",
       "       BNS    80.0   320.0 ... 5.768320995793772    0.7037123471233048\n",
       "      NSBH    61.0   130.0 ... 4.867534450455582   0.41890166985175514\n",
       "       BBH    15.3    23.9 ... 3.173878458937465    0.2781349878864127"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rates_table[\"fiducial_log_rate\"] = fiducial_log_rates\n",
    "rates_table[\"fiducial_log_rate_err\"] = fiducial_log_rate_errs\n",
    "\n",
    "rates_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "f326e152",
   "metadata": {},
   "outputs": [],
   "source": [
    "prob_quantiles = np.asarray([0.05, 0.5, 0.95])\n",
    "run_duration = 1.0  # years"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "40c16d78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.330892345522338 34.3640927188856 111.63172036081173\n",
      "34.05143319981825 72.0894882442789 146.04995420420497\n",
      "64.03864903155096 105.93977687481468 170.6130780949588\n",
      "57.7025770477992 188.39270153379962 601.7266733340127\n",
      "179.43478574585095 361.7728800589275 723.0345185131331\n",
      "298.3272702572061 476.30088523036954 755.8459665889585\n"
     ]
    }
   ],
   "source": [
    "results = {}\n",
    "for run_name in run_names:\n",
    "    results[run_name] = {}\n",
    "\n",
    "    for pop in [\"BNS\", \"NSBH\", \"BBH\"]:\n",
    "        rates_row = rates_table[rates_table[\"population\"] == pop]\n",
    "        rate = rates_row[f\"sim_rate_{run_name}\"]  # here  rates_row['mass_fraction'] = 1\n",
    "\n",
    "        mu = (\n",
    "            rates_row[\"fiducial_log_rate\"]\n",
    "            + np.log(run_duration)\n",
    "            + np.log(rates_row[f\"detection_number_{run_name}\"] / rate)\n",
    "        )\n",
    "        sigma = rates_row[\"fiducial_log_rate_err\"]\n",
    "\n",
    "        lo, mid, hi = poisson_lognormal_rate_quantiles(prob_quantiles, mu, sigma)\n",
    "        print(lo, mid, hi)\n",
    "        lo = int(np.floor(lo))\n",
    "        mid = int(np.round(mid))\n",
    "        hi = int(np.ceil(hi))\n",
    "\n",
    "        mid, lo, hi = format_with_errorbars(mid, lo, hi)\n",
    "\n",
    "        results[run_name].setdefault(\"low\", {})[pop] = lo\n",
    "        results[run_name].setdefault(\"mid\", {})[pop] = mid\n",
    "        results[run_name].setdefault(\"high\", {})[pop] = hi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "342ec999",
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = []\n",
    "for run_name, run_stats in results.items():\n",
    "    for pop in run_stats[\"mid\"]:\n",
    "        rows.append(\n",
    "            {\n",
    "                \"run\": run_name,\n",
    "                \"population\": pop,\n",
    "                \"low\": f\"-{run_stats['low'][pop]}\",\n",
    "                \"mid\": run_stats[\"mid\"][pop],\n",
    "                \"high\": f\"+{run_stats['high'][pop]}\",\n",
    "            }\n",
    "        )\n",
    "\n",
    "results_table = Table(rows=rows, names=(\"run\", \"population\", \"low\", \"mid\", \"high\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "5d25c1f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><i>Table length=6</i>\n",
       "<table id=\"table7215938128\" class=\"table-striped table-bordered table-condensed\">\n",
       "<thead><tr><th>run</th><th>population</th><th>low</th><th>mid</th><th>high</th></tr></thead>\n",
       "<thead><tr><th>str2</th><th>str4</th><th>str4</th><th>str3</th><th>str4</th></tr></thead>\n",
       "<tr><td>O4</td><td>BNS</td><td>-25</td><td>34</td><td>+78</td></tr>\n",
       "<tr><td>O4</td><td>NSBH</td><td>-38</td><td>72</td><td>+75</td></tr>\n",
       "<tr><td>O4</td><td>BBH</td><td>-42</td><td>106</td><td>+65</td></tr>\n",
       "<tr><td>O5</td><td>BNS</td><td>-130</td><td>190</td><td>+410</td></tr>\n",
       "<tr><td>O5</td><td>NSBH</td><td>-180</td><td>360</td><td>+360</td></tr>\n",
       "<tr><td>O5</td><td>BBH</td><td>-180</td><td>480</td><td>+280</td></tr>\n",
       "</table></div>"
      ],
      "text/plain": [
       "<Table length=6>\n",
       "run  population low  mid  high\n",
       "str2    str4    str4 str3 str4\n",
       "---- ---------- ---- ---- ----\n",
       "  O4        BNS  -25   34  +78\n",
       "  O4       NSBH  -38   72  +75\n",
       "  O4        BBH  -42  106  +65\n",
       "  O5        BNS -130  190 +410\n",
       "  O5       NSBH -180  360 +360\n",
       "  O5        BBH -180  480 +280"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "15d54825",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Observing Run  O4\n",
      "==============================\n",
      "run population low mid high\n",
      "--- ---------- --- --- ----\n",
      " O4        BNS -25  34  +78\n",
      " O4       NSBH -38  72  +75\n",
      " O4        BBH -42 106  +65\n",
      "==============================\n",
      "\n",
      " Observing Run  O5\n",
      "==============================\n",
      "run population low  mid high\n",
      "--- ---------- ---- --- ----\n",
      " O5        BNS -130 190 +410\n",
      " O5       NSBH -180 360 +360\n",
      " O5        BBH -180 480 +280\n",
      "==============================\n"
     ]
    }
   ],
   "source": [
    "# display by run\n",
    "for run_name in sorted(set(results_table[\"run\"])):\n",
    "    print(f\"\\n Observing Run  {run_name}\")\n",
    "    print(\"=\" * 30)\n",
    "    print(results_table[results_table[\"run\"] == run_name])\n",
    "    print(\"=\" * 30)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tilepy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
